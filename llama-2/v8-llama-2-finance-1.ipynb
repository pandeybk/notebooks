{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526fb9d2-d6a4-4072-9baa-74faa9a86b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "  ### Question:\n",
    "  {query}\n",
    "\n",
    "  ### Answer:\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9c02a5-245f-412b-92ff-755c6f2e2522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib/python3.9/site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.2\n",
      "    Uninstalling pip-22.2.2:\n",
      "      Successfully uninstalled pip-22.2.2\n",
      "Successfully installed pip-23.3.1\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl.metadata (9.8 kB)\n",
      "Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.41.2.post2\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-al9px812\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-al9px812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -U bitsandbytes\n",
    "!pip install  -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install  -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install datasets\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb7ca9-729e-43cb-8308-b02bbdcf8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(\"Number of GPU:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(torch.cuda.get_device_properties(i))\n",
    "        print(f\"Memory GB: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"GPU Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"GPU Cached:    {torch.cuda.memory_reserved(i) / 1024 ** 3:.2f} GB\")\n",
    "        \n",
    "        \n",
    "else:\n",
    "    print(\"CUDA is not available. No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76acbdc-a175-4d6d-bcbd-d6a6915155f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308d105-58ae-43ac-9ee4-6439ccaebc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)  # Show all columns in the DataFrame\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure the full content of each cell is displayed\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset(\"gbharti/finance-alpaca\", split='train')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = data.to_pandas()\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1dae57-6c9c-45de-8c99-a07e285283f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and it's already loaded\n",
    "\n",
    "# Basic Dataset Information\n",
    "print(\"Basic Dataset Information:\")\n",
    "print(f\"Number of Rows: {df.shape[0]}\")\n",
    "print(f\"Number of Columns: {df.shape[1]}\")\n",
    "print(f\"Column Names: {df.columns.tolist()}\", end=\"\\n\\n\")\n",
    "\n",
    "# Memory Usage\n",
    "print(\"Memory Usage by Column:\")\n",
    "print(df.memory_usage(deep=True), end=\"\\n\\n\")\n",
    "\n",
    "# Data Types\n",
    "print(\"Data Types of Each Column:\")\n",
    "print(df.dtypes, end=\"\\n\\n\")\n",
    "\n",
    "# Calculating the length of each cell in each column\n",
    "analysis_df = df.copy()\n",
    "analysis_df['num_characters_instruction'] = analysis_df['instruction'].apply(len)\n",
    "analysis_df['num_characters_input'] = analysis_df['input'].apply(len)\n",
    "analysis_df['num_characters_output'] = analysis_df['output'].apply(len)\n",
    "\n",
    "# Show Distribution\n",
    "analysis_df.hist(column=['num_characters_instruction', 'num_characters_input', 'num_characters_output'], bins=30, figsize=(12, 8))\n",
    "plt.suptitle('Distribution of Character Counts in Each Column')\n",
    "plt.show()\n",
    "\n",
    "# Descriptive Statistics for Character Counts\n",
    "print(\"Descriptive Statistics for Character Counts:\")\n",
    "print(analysis_df[['num_characters_instruction', 'num_characters_input', 'num_characters_output']].describe(), end=\"\\n\\n\")\n",
    "\n",
    "# Additional Detailed Statistics\n",
    "max_chars_instruction = analysis_df['num_characters_instruction'].max()\n",
    "max_chars_input = analysis_df['num_characters_input'].max()\n",
    "max_chars_output = analysis_df['num_characters_output'].max()\n",
    "\n",
    "min_chars_instruction = analysis_df['num_characters_instruction'].min()\n",
    "min_chars_input = analysis_df['num_characters_input'].min()\n",
    "min_chars_output = analysis_df['num_characters_output'].min()\n",
    "\n",
    "# Print detailed statistics\n",
    "# Missing Values\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(analysis_df.isnull().sum(), end=\"\\n\\n\")\n",
    "\n",
    "# Unique Values\n",
    "print(\"Unique Values in Each Column:\")\n",
    "print(analysis_df.nunique(), end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac61500-8a23-44a5-ad87-5de1dd39ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7aae13-f176-4b35-8f97-459c38e359ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153a46a-4104-4efe-81a8-d4c81c98aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_completion(query=\"Will capital gains affect my tax bracket?\", model=base_model, tokenizer=base_tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8abb82-66ef-4384-b412-9043b86a4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "    :param data_point: dict: Data point\n",
    "    :return: dict: tokenzed prompt\n",
    "    \"\"\"\n",
    "    # Samples with additional context into.\n",
    "    if data_point['input']:\n",
    "        text = 'Below is an instruction that describes a task, paired with an input that provides' \\\n",
    "               ' further context. Write a response that appropriately completes the request.\\n\\n'\n",
    "        text += f'### Instruction:\\n{data_point[\"instruction\"]}\\n\\n'\n",
    "        text += f'### Input:\\n{data_point[\"input\"]}\\n\\n'\n",
    "        text += f'### Response:\\n{data_point[\"output\"]}'\n",
    "\n",
    "    # Without\n",
    "    else:\n",
    "        text = 'Below is an instruction that describes a task. Write a response that ' \\\n",
    "               'appropriately completes the request.\\n\\n'\n",
    "        text += f'### Instruction:\\n{data_point[\"instruction\"]}\\n\\n'\n",
    "        text += f'### Response:\\n{data_point[\"output\"]}'\n",
    "    return text\n",
    "\n",
    "# add the \"prompt\" column in the dataset\n",
    "text_column = [generate_prompt(data_point) for data_point in data]\n",
    "data = data.add_column(\"prompt\", text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8749e-2236-4a3b-b40c-b504e2949505",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.shuffle(seed=1234)  # Shuffle dataset here\n",
    "data = data.map(lambda samples: base_tokenizer(samples[\"prompt\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23fb0e-efc0-4952-8da6-615f8c935975",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.train_test_split(test_size=0.1)\n",
    "train_data = data[\"train\"]\n",
    "test_data = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e904f94-80ef-4130-bf9b-8e6699aa84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3bf0c-ae44-4d0c-abc4-f1670c7b5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model = prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4e18d-6ad1-4754-ab4a-d0c7ba392097",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058491cf-6da9-4a4a-9861-60f5d83a7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29616afa-b38e-4a8c-958f-9ead1c349fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = find_all_linear_names(base_model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e8ce3-d94d-4387-b634-6c1920cbfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "base_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e49ed-875f-4b25-a6d6-4480c4f40b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable, total = base_model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a858c-687f-4cb6-a669-1c808b27b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed58c9-9515-4f13-b18e-e4a3c9a91adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install  trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1ea29-e992-4173-80ee-f0b8f83a6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=50, #Absolute number of warmup steps\n",
    "        max_steps=1000,\n",
    "        learning_rate=1e-5,\n",
    "        # logging_dir=\"./logs\",\n",
    "        logging_first_step=True,\n",
    "        logging_steps=20,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        optim=\"adamw_torch\",\n",
    "        eval_steps=50,\n",
    "        output_dir=\"outputs\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(base_tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a7647-b093-47eb-83fc-5d897e320d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=50, #Absolute number of warmup steps\n",
    "        max_steps=1000,\n",
    "        learning_rate=1e-5,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_first_step=True,\n",
    "        logging_steps=20,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        optim=\"adamw_torch\",\n",
    "        eval_steps=50,\n",
    "        output_dir=\"outputs\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(base_tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc80bdb-f837-4283-ad93-b28346a47c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train(\"./outputs/checkpoint-1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6772e-ade0-472e-b76e-744f43d0787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.push_to_hub(\"Llama-2-7b-hf_finetuned_finance_jupyter_v5\")\n",
    "base_tokenizer.push_to_hub(\"Llama-2-7b-hf_finetuned_finance_jupyter_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ef37c-1b3e-4f7d-b3f3-734816a3a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"bkpandey/Llama-2-7b-hf_finetuned_finance_jupyter_v5\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_4bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ad9d2-6313-4ee1-87c4-7d5e73aa0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_completion(query=\"Will capital gains affect my tax bracket?\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55696e51-f3ba-4c36-97d1-80aa5e3fcb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
