{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511d4035-de3d-4ec5-a90b-1657610c832e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # \"True\" is to restart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18fa936-90a4-436d-a488-77b03f6a9e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.14.6 in /opt/app-root/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.14.6)\n",
      "Requirement already satisfied: peft==0.4.0 in /opt/app-root/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: torch==2.0.1 in /opt/app-root/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: transformers==4.31.0 in /opt/app-root/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (4.31.0)\n",
      "Requirement already satisfied: trl==0.4.7 in /opt/app-root/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.4.7)\n",
      "Requirement already satisfied: scipy in /opt/app-root/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.11.3)\n",
      "Requirement already satisfied: tiktoken in /opt/app-root/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.5.1)\n",
      "Requirement already satisfied: bitsandbytes in /opt/app-root/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.41.2.post2)\n",
      "Requirement already satisfied: accelerate in /opt/app-root/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.24.1)\n",
      "Requirement already satisfied: multiprocess in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (0.70.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (1.26.1)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (0.19.0)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (14.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (3.8.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: safetensors in /opt/app-root/lib/python3.9/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.9.5)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (11.7.91)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (10.2.10.91)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (2.14.3)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (11.7.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 4)) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 4)) (0.13.3)\n",
      "Requirement already satisfied: wheel in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 3)) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 3)) (53.0.0)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 3)) (3.27.7)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 3)) (17.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets==2.14.6->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets==2.14.6->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets==2.14.6->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6->-r requirements.txt (line 1)) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe69ef8-7a5a-4761-a234-363b232cda82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/app-root/lib/python3.9/site-packages (0.24.1)\n",
      "Requirement already satisfied: bitsandbytes in /opt/app-root/lib/python3.9/site-packages (0.41.2.post2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/app-root/lib/python3.9/site-packages (from accelerate) (0.19.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/app-root/lib/python3.9/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: pyyaml in /opt/app-root/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: wheel in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (53.0.0)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.7)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install  accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2601458b-6bfd-4674-88cd-c9740308bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e2e6714-a94d-4362-a9e1-3c4f3c0fe11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Available CUDA devices: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n"
     ]
    }
   ],
   "source": [
    "available_gpus = [f'cuda:{i}' for i in range(torch.cuda.device_count())]\n",
    "logger.info(f\"Available CUDA devices: {available_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5ff9a7f-2395-4c77-8835-aee186218ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"daryl149/Llama-2-7b-hf\" \n",
    "dataset_name = \"tatsu-lab/alpaca\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14c241ab-46f8-49e4-a66a-77572874ce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = load_dataset(dataset_name, split=\"train[0:10000]\")\n",
    "logger.info(\"Dataset loaded successfully.\")\n",
    "\n",
    "data = data.train_test_split(test_size=0.1)\n",
    "train_dataset = data[\"train\"]\n",
    "test_dataset = data[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d519b64-dd4d-4680-b612-ed4bdf68c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Analyze the given poem and briefly explain its main message.', 'input': \"The poem reads as follows:\\n\\nThe world will keep spinning\\nAs it always has been\\nBut that doesn't mean that you,\\nAre ever really unseen\", 'output': 'The poem conveys the message that everyone has unique value and is an irreplaceable part of the world, even though the world carries on as it always has. The poem reminds the reader not to take for granted the importance of their individual presence and contribution in life.', 'text': \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnalyze the given poem and briefly explain its main message.\\n\\n### Input:\\nThe poem reads as follows:\\n\\nThe world will keep spinning\\nAs it always has been\\nBut that doesn't mean that you,\\nAre ever really unseen\\n\\n### Response:\\nThe poem conveys the message that everyone has unique value and is an irreplaceable part of the world, even though the world carries on as it always has. The poem reminds the reader not to take for granted the importance of their individual presence and contribution in life.\"}\n",
      "{'instruction': 'Given a user query, suggest a relevant title.', 'input': 'Query: Best hotels in London', 'output': '\"Top-Rated Hotels: An Ultimate Guide to London\"', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven a user query, suggest a relevant title.\\n\\n### Input:\\nQuery: Best hotels in London\\n\\n### Response:\\n\"Top-Rated Hotels: An Ultimate Guide to London\"'}\n",
      "{'instruction': 'Add at least 2 more examples to this list: flight, eagle, hawk', 'input': '', 'output': 'Osprey, Falcon, Kite.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAdd at least 2 more examples to this list: flight, eagle, hawk\\n\\n### Response:\\nOsprey, Falcon, Kite.'}\n",
      "{'instruction': 'Given the following list of numbers, identify the median.', 'input': 'List: [3, 5, 19, 25, 32]', 'output': 'The median of [3, 5, 19, 25, 32] is 19.', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven the following list of numbers, identify the median.\\n\\n### Input:\\nList: [3, 5, 19, 25, 32]\\n\\n### Response:\\nThe median of [3, 5, 19, 25, 32] is 19.'}\n",
      "{'instruction': 'Oragnize the following statement into a proper syntax.', 'input': 'Mike yesterday to go running but the drizzle rain stopped', 'output': 'Yesterday, Mike was going to go running, but the drizzle rain stopped him.', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nOragnize the following statement into a proper syntax.\\n\\n### Input:\\nMike yesterday to go running but the drizzle rain stopped\\n\\n### Response:\\nYesterday, Mike was going to go running, but the drizzle rain stopped him.'}\n",
      "{'instruction': 'Generate three expansion sentences of the given sentence.', 'input': 'The app is easy to use.', 'output': \"1. The app is designed to be user-friendly and intuitive.\\n2. It's easy for users to navigate the app. \\n3. It only takes a few minutes to learn how to use the app.\", 'text': \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate three expansion sentences of the given sentence.\\n\\n### Input:\\nThe app is easy to use.\\n\\n### Response:\\n1. The app is designed to be user-friendly and intuitive.\\n2. It's easy for users to navigate the app. \\n3. It only takes a few minutes to learn how to use the app.\"}\n",
      "{'instruction': 'Write a headline summarizing the article in one sentence.', 'input': 'The article discusses the growing trend of plant-based diets and their potential health benefits.', 'output': 'Plant-Based Diets Gaining Popularity Among Health-Conscious Consumers.', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a headline summarizing the article in one sentence.\\n\\n### Input:\\nThe article discusses the growing trend of plant-based diets and their potential health benefits.\\n\\n### Response:\\nPlant-Based Diets Gaining Popularity Among Health-Conscious Consumers.'}\n",
      "{'instruction': 'Select the most appropriate word from the list.', 'input': 'joyful, joyous, festive', 'output': 'joyous', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSelect the most appropriate word from the list.\\n\\n### Input:\\njoyful, joyous, festive\\n\\n### Response:\\njoyous'}\n",
      "{'instruction': 'Generate a list of ways to foster creativity in the workplace.', 'input': '', 'output': 'Here are some ways to foster creativity in the workplace:\\n- Provide opportunities for team building and collaboration.\\n- Encourage original and out of the box thinking.\\n- Allow for brainstorming sessions and fresh ideas.\\n- Provide flexible hours for employees to work.\\n- Allow for a mix of tasks and assignments.\\n- Promote a culture of risk-taking and experimentation.\\n- Invite guest speakers and industry experts.\\n- Offer creativity and innovation workshops.\\n- Allow for controlled trial and error.\\n- Foster an atmosphere of collaboration and respect.\\n- Encourage mentorship and feedback.\\n- Sponsor creative competitions and contests.\\n- Provide recognition and rewards for creative ideas.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a list of ways to foster creativity in the workplace.\\n\\n### Response:\\nHere are some ways to foster creativity in the workplace:\\n- Provide opportunities for team building and collaboration.\\n- Encourage original and out of the box thinking.\\n- Allow for brainstorming sessions and fresh ideas.\\n- Provide flexible hours for employees to work.\\n- Allow for a mix of tasks and assignments.\\n- Promote a culture of risk-taking and experimentation.\\n- Invite guest speakers and industry experts.\\n- Offer creativity and innovation workshops.\\n- Allow for controlled trial and error.\\n- Foster an atmosphere of collaboration and respect.\\n- Encourage mentorship and feedback.\\n- Sponsor creative competitions and contests.\\n- Provide recognition and rewards for creative ideas.'}\n",
      "{'instruction': 'Create an appropriate user interface for an online store that enables a user to buy items.', 'input': '', 'output': 'The user interface for the online store should be simple, intuitive and easy to use. It should enable the user to easily search for products, browse through product categories and compare different items. The interface should also enable the user to view product details such as images, prices and related items. It should enable the user to add items to their cart, select payment methods and provide an easy to use checkout process. Additionally, the user should be able to review their orders and track shipments.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate an appropriate user interface for an online store that enables a user to buy items.\\n\\n### Response:\\nThe user interface for the online store should be simple, intuitive and easy to use. It should enable the user to easily search for products, browse through product categories and compare different items. The interface should also enable the user to view product details such as images, prices and related items. It should enable the user to add items to their cart, select payment methods and provide an easy to use checkout process. Additionally, the user should be able to review their orders and track shipments.'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "864ffe06-5249-40dd-9a44-ff5879ed6a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Analyze the given poem and briefly explain its main message.\n",
      "Input: The poem reads as follows:\n",
      "\n",
      "The world will keep spinning\n",
      "As it always has been\n",
      "But that doesn't mean that you,\n",
      "Are ever really unseen\n",
      "Output: .\n",
      "==================================================\n",
      "Instruction: Given a user query, suggest a relevant title.\n",
      "Input: Query: Best hotels in London\n",
      "Output: . Response: The Ritz London, The Savoy, The Langham, The Dorchester, The Connaught, The Savoy, The Goring,\n",
      "==================================================\n",
      "Instruction: Add at least 2 more examples to this list: flight, eagle, hawk\n",
      "Input: \n",
      "Output: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Des\n",
      "==================================================\n",
      "Instruction: Given the following list of numbers, identify the median.\n",
      "Input: List: [3, 5, 19, 25, 32]\n",
      "Output: \n",
      "==================================================\n",
      "Instruction: Oragnize the following statement into a proper syntax.\n",
      "Input: Mike yesterday to go running but the drizzle rain stopped\n",
      "Output: him.\n",
      "\n",
      "### Input:\n",
      "Mike yesterday to go running but the drizzle rain stopped him.\n",
      "==================================================\n",
      "Instruction: Generate three expansion sentences of the given sentence.\n",
      "Input: The app is easy to use.\n",
      "Output: ### Input:\n",
      "The cat is sleeping.\n",
      "\n",
      "### Response:\n",
      "The cat is sleeping on the couch.\n",
      "The cat\n",
      "==================================================\n",
      "Instruction: Write a headline summarizing the article in one sentence.\n",
      "Input: The article discusses the growing trend of plant-based diets and their potential health benefits.\n",
      "Output: ### Response:\n",
      "Plant-based diets are becoming increasingly popular\n",
      "==================================================\n",
      "Instruction: Select the most appropriate word from the list.\n",
      "Input: joyful, joyous, festive\n",
      "Output: , merry, jolly, jovial, jubilant, exultant, ecstatic, elated, blissful, blithe\n",
      "==================================================\n",
      "Instruction: Generate a list of ways to foster creativity in the workplace.\n",
      "Input: \n",
      "Output: ### Response:\n",
      "1. Encourage employees to take breaks and engage in activities that stimulate creativity, such as reading, listening to\n",
      "==================================================\n",
      "Instruction: Create an appropriate user interface for an online store that enables a user to buy items.\n",
      "Input: \n",
      "Output: ### Response:\n",
      "The user interface for an online store should include a search bar, a list of products, and a shopping cart. The\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline for text generation with the model and tokenizer\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def generate_model_output(instruction, input_text, generator):\n",
    "    # Combine instruction and input text to form the prompt for the model\n",
    "    prompt = f\"{instruction} {input_text}\"\n",
    "    \n",
    "    # Generate output using the model pipeline\n",
    "    outputs = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "    \n",
    "    # Extract the generated text and return it\n",
    "    # Assuming that the model's output starts after the input text\n",
    "    # You may need to adjust slicing depending on your model's behavior\n",
    "    generated_text = outputs[0][\"generated_text\"][len(prompt):]\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "# Example usage\n",
    "for i in range(10):\n",
    "    instruction = data[\"train\"][\"instruction\"][i]\n",
    "    input_text = data[\"train\"][\"input\"][i]\n",
    "    \n",
    "    # Get the model's output\n",
    "    output = generate_model_output(instruction, input_text, generator)\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(\"=\" * 50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e00aa14b-7b6c-4d35-9024-d84566d2141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Write a summary of the following text:\n",
      "Input: The history of AI dates back to antiquity. Ancient myths about artificial beings endowed with intelligence or consciousness have been common throughout history.\n",
      "Output: In the 19th century, the\n",
      "==================================================\n",
      "Instruction: Translate the following sentence to French:\n",
      "Input: Hello, how are you today?\n",
      "Output: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Create a list of 5\n",
      "==================================================\n",
      "Instruction: Answer the question based on the passage provided:\n",
      "Input: Passage: 'The quick brown fox jumps over the lazy dog.' Question: 'Which animal is quick?'\n",
      "Output: Response: The quick brown fox jumps over the lazy dog. Below is\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Example instructions and inputs\n",
    "example_data = [\n",
    "    {\n",
    "        \"instruction\": \"Write a summary of the following text:\",\n",
    "        \"input\": \"The history of AI dates back to antiquity. Ancient myths about artificial beings endowed with intelligence or consciousness have been common throughout history.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Translate the following sentence to French:\",\n",
    "        \"input\": \"Hello, how are you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Answer the question based on the passage provided:\",\n",
    "        \"input\": \"Passage: 'The quick brown fox jumps over the lazy dog.' Question: 'Which animal is quick?'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the generate_model_output function with the examples\n",
    "for example in example_data:\n",
    "    output = generate_model_output(example[\"instruction\"], example[\"input\"], generator)\n",
    "    print(f\"Instruction: {example['instruction']}\")\n",
    "    print(f\"Input: {example['input']}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(\"=\" * 50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c790879-73a0-426c-95b7-1bd458264bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Tokenizer prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "logger.info(\"Tokenizer prepared successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e7bb0f4-3679-4a34-95ad-3832009ff80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)9/llama-2-7b-hf/resolve/main/config.json: 100%|██████████| 578/578 [00:00<00:00, 158kB/s]\n",
      "(…)esolve/main/pytorch_model.bin.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 14.5MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   0%|          | 10.5M/9.98G [00:00<02:28, 67.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   0%|          | 31.5M/9.98G [00:00<01:19, 126MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   1%|          | 52.4M/9.98G [00:00<01:03, 156MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   1%|          | 73.4M/9.98G [00:00<00:58, 170MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   1%|          | 94.4M/9.98G [00:00<00:54, 181MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   1%|          | 115M/9.98G [00:00<00:52, 188MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   1%|▏         | 136M/9.98G [00:00<00:50, 193MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   2%|▏         | 157M/9.98G [00:00<00:49, 196MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   2%|▏         | 178M/9.98G [00:01<00:49, 198MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   2%|▏         | 199M/9.98G [00:01<00:48, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   2%|▏         | 220M/9.98G [00:01<00:48, 202MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   2%|▏         | 241M/9.98G [00:01<00:48, 202MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   3%|▎         | 262M/9.98G [00:01<00:47, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   3%|▎         | 283M/9.98G [00:01<00:47, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   3%|▎         | 304M/9.98G [00:01<00:47, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   3%|▎         | 325M/9.98G [00:01<00:47, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   3%|▎         | 346M/9.98G [00:01<00:47, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   4%|▎         | 367M/9.98G [00:01<00:47, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   4%|▍         | 388M/9.98G [00:02<00:46, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   4%|▍         | 409M/9.98G [00:02<00:46, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   4%|▍         | 430M/9.98G [00:02<00:46, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   5%|▍         | 451M/9.98G [00:02<00:46, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   5%|▍         | 472M/9.98G [00:02<00:46, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   5%|▍         | 493M/9.98G [00:02<00:47, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   5%|▌         | 514M/9.98G [00:02<00:46, 202MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   5%|▌         | 535M/9.98G [00:02<00:46, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   6%|▌         | 556M/9.98G [00:02<00:46, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   6%|▌         | 577M/9.98G [00:02<00:46, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   6%|▌         | 598M/9.98G [00:03<00:45, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   6%|▌         | 619M/9.98G [00:03<00:45, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   6%|▋         | 640M/9.98G [00:03<00:46, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   7%|▋         | 661M/9.98G [00:03<00:46, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   7%|▋         | 682M/9.98G [00:03<00:46, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   7%|▋         | 703M/9.98G [00:03<00:46, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   7%|▋         | 724M/9.98G [00:03<00:46, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   7%|▋         | 744M/9.98G [00:03<00:46, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   8%|▊         | 765M/9.98G [00:03<00:46, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   8%|▊         | 786M/9.98G [00:04<00:45, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   8%|▊         | 807M/9.98G [00:04<00:46, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   8%|▊         | 828M/9.98G [00:04<00:45, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   9%|▊         | 849M/9.98G [00:04<00:46, 198MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   9%|▊         | 870M/9.98G [00:04<00:45, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   9%|▉         | 891M/9.98G [00:04<00:45, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   9%|▉         | 912M/9.98G [00:04<00:45, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:   9%|▉         | 933M/9.98G [00:04<00:45, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  10%|▉         | 954M/9.98G [00:04<00:45, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  10%|▉         | 975M/9.98G [00:04<00:44, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  10%|▉         | 996M/9.98G [00:05<00:44, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  10%|█         | 1.02G/9.98G [00:05<00:45, 197MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  10%|█         | 1.04G/9.98G [00:05<00:44, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  11%|█         | 1.06G/9.98G [00:05<00:45, 198MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  11%|█         | 1.08G/9.98G [00:05<00:44, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  11%|█         | 1.10G/9.98G [00:05<00:44, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  11%|█         | 1.12G/9.98G [00:05<00:44, 197MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  11%|█▏        | 1.14G/9.98G [00:05<00:44, 198MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  12%|█▏        | 1.16G/9.98G [00:05<00:44, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  12%|█▏        | 1.18G/9.98G [00:06<00:43, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  12%|█▏        | 1.21G/9.98G [00:06<00:46, 189MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  12%|█▏        | 1.24G/9.98G [00:06<00:43, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  13%|█▎        | 1.27G/9.98G [00:06<00:42, 207MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  13%|█▎        | 1.30G/9.98G [00:06<00:40, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  13%|█▎        | 1.33G/9.98G [00:06<00:39, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  14%|█▎        | 1.36G/9.98G [00:06<00:39, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  14%|█▍        | 1.39G/9.98G [00:07<00:41, 209MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  14%|█▍        | 1.43G/9.98G [00:07<00:40, 209MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  15%|█▍        | 1.45G/9.98G [00:07<00:41, 207MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  15%|█▍        | 1.47G/9.98G [00:07<00:41, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  15%|█▌        | 1.50G/9.98G [00:07<00:40, 208MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  15%|█▌        | 1.53G/9.98G [00:07<00:40, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  16%|█▌        | 1.56G/9.98G [00:07<00:39, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  16%|█▌        | 1.59G/9.98G [00:07<00:39, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  16%|█▋        | 1.63G/9.98G [00:08<00:38, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  17%|█▋        | 1.66G/9.98G [00:08<00:38, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  17%|█▋        | 1.69G/9.98G [00:08<00:39, 208MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  17%|█▋        | 1.72G/9.98G [00:08<00:39, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  18%|█▊        | 1.75G/9.98G [00:08<00:39, 211MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  18%|█▊        | 1.78G/9.98G [00:08<00:39, 208MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  18%|█▊        | 1.81G/9.98G [00:09<00:38, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  18%|█▊        | 1.85G/9.98G [00:09<00:38, 209MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  19%|█▊        | 1.87G/9.98G [00:09<00:39, 207MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  19%|█▉        | 1.89G/9.98G [00:09<00:40, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  19%|█▉        | 1.92G/9.98G [00:09<00:38, 207MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  20%|█▉        | 1.95G/9.98G [00:09<00:37, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  20%|█▉        | 1.98G/9.98G [00:09<00:37, 211MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  20%|██        | 2.01G/9.98G [00:09<00:37, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  20%|██        | 2.04G/9.98G [00:10<00:36, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  21%|██        | 2.08G/9.98G [00:10<00:41, 193MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  21%|██        | 2.11G/9.98G [00:10<00:39, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  21%|██▏       | 2.14G/9.98G [00:10<00:37, 207MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  22%|██▏       | 2.17G/9.98G [00:10<00:36, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  22%|██▏       | 2.20G/9.98G [00:10<00:41, 186MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  22%|██▏       | 2.22G/9.98G [00:11<01:02, 125MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  22%|██▏       | 2.24G/9.98G [00:11<01:16, 102MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|██▎       | 2.26G/9.98G [00:11<01:29, 85.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|██▎       | 2.29G/9.98G [00:12<01:43, 74.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|██▎       | 2.30G/9.98G [00:12<01:46, 72.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|██▎       | 2.31G/9.98G [00:12<01:50, 69.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|██▎       | 2.32G/9.98G [00:12<01:50, 69.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|██▎       | 2.33G/9.98G [00:13<01:56, 65.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  23%|██▎       | 2.34G/9.98G [00:13<01:52, 67.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|██▎       | 2.35G/9.98G [00:13<01:57, 65.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|██▎       | 2.36G/9.98G [00:13<02:18, 55.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|██▍       | 2.37G/9.98G [00:13<02:32, 49.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|██▍       | 2.38G/9.98G [00:14<02:18, 54.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|██▍       | 2.39G/9.98G [00:14<02:02, 61.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|██▍       | 2.41G/9.98G [00:14<01:26, 87.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|██▍       | 2.42G/9.98G [00:14<01:27, 86.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|██▍       | 2.43G/9.98G [00:14<01:25, 88.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  24%|██▍       | 2.44G/9.98G [00:14<01:28, 84.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  25%|██▍       | 2.47G/9.98G [00:14<00:59, 126MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  25%|██▌       | 2.51G/9.98G [00:14<00:48, 153MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  25%|██▌       | 2.54G/9.98G [00:15<00:42, 173MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|██▌       | 2.57G/9.98G [00:15<00:39, 186MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|██▌       | 2.60G/9.98G [00:15<00:37, 196MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  26%|██▋       | 2.62G/9.98G [00:15<00:37, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  27%|██▋       | 2.65G/9.98G [00:15<00:35, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  27%|██▋       | 2.67G/9.98G [00:15<00:35, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  27%|██▋       | 2.71G/9.98G [00:15<00:34, 211MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  27%|██▋       | 2.74G/9.98G [00:16<00:34, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  28%|██▊       | 2.77G/9.98G [00:16<00:49, 145MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  28%|██▊       | 2.80G/9.98G [00:16<00:44, 162MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  28%|██▊       | 2.83G/9.98G [00:16<00:40, 176MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  29%|██▊       | 2.86G/9.98G [00:16<00:38, 187MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  29%|██▉       | 2.89G/9.98G [00:16<00:36, 195MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  29%|██▉       | 2.93G/9.98G [00:17<00:35, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  30%|██▉       | 2.96G/9.98G [00:17<00:34, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  30%|██▉       | 2.98G/9.98G [00:17<00:34, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  30%|███       | 3.00G/9.98G [00:17<00:34, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  30%|███       | 3.02G/9.98G [00:17<00:34, 200MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  30%|███       | 3.04G/9.98G [00:17<00:34, 198MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  31%|███       | 3.06G/9.98G [00:17<00:35, 196MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  31%|███       | 3.08G/9.98G [00:17<00:34, 197MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  31%|███       | 3.11G/9.98G [00:18<00:33, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  32%|███▏      | 3.15G/9.98G [00:18<00:33, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  32%|███▏      | 3.17G/9.98G [00:18<00:33, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  32%|███▏      | 3.19G/9.98G [00:18<00:33, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  32%|███▏      | 3.22G/9.98G [00:18<00:32, 208MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  33%|███▎      | 3.25G/9.98G [00:18<00:31, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  33%|███▎      | 3.28G/9.98G [00:18<00:31, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  33%|███▎      | 3.31G/9.98G [00:18<00:30, 219MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  34%|███▎      | 3.34G/9.98G [00:19<00:30, 220MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  34%|███▍      | 3.38G/9.98G [00:19<00:30, 220MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  34%|███▍      | 3.41G/9.98G [00:19<00:29, 219MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  34%|███▍      | 3.44G/9.98G [00:19<00:30, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  35%|███▍      | 3.47G/9.98G [00:19<00:29, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  35%|███▌      | 3.50G/9.98G [00:19<00:29, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  35%|███▌      | 3.53G/9.98G [00:19<00:29, 219MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  36%|███▌      | 3.57G/9.98G [00:20<00:29, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  36%|███▌      | 3.60G/9.98G [00:20<00:29, 219MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  36%|███▋      | 3.63G/9.98G [00:20<00:29, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  37%|███▋      | 3.66G/9.98G [00:20<00:29, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  37%|███▋      | 3.69G/9.98G [00:20<00:29, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  37%|███▋      | 3.72G/9.98G [00:20<00:29, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  38%|███▊      | 3.75G/9.98G [00:20<00:28, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  38%|███▊      | 3.79G/9.98G [00:21<00:28, 219MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  38%|███▊      | 3.82G/9.98G [00:21<00:28, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  39%|███▊      | 3.85G/9.98G [00:21<00:28, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  39%|███▉      | 3.88G/9.98G [00:21<00:27, 219MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  39%|███▉      | 3.91G/9.98G [00:21<00:27, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  40%|███▉      | 3.94G/9.98G [00:21<00:27, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  40%|███▉      | 3.97G/9.98G [00:21<00:27, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  40%|████      | 4.01G/9.98G [00:22<00:27, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  40%|████      | 4.04G/9.98G [00:22<00:27, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  41%|████      | 4.07G/9.98G [00:22<00:27, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  41%|████      | 4.10G/9.98G [00:22<00:27, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  41%|████▏     | 4.13G/9.98G [00:22<00:26, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  42%|████▏     | 4.16G/9.98G [00:22<00:26, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  42%|████▏     | 4.19G/9.98G [00:23<00:26, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  42%|████▏     | 4.23G/9.98G [00:23<00:27, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  43%|████▎     | 4.26G/9.98G [00:23<00:27, 209MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  43%|████▎     | 4.28G/9.98G [00:23<00:27, 209MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  43%|████▎     | 4.30G/9.98G [00:23<00:27, 208MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  43%|████▎     | 4.33G/9.98G [00:23<00:26, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  44%|████▎     | 4.36G/9.98G [00:23<00:26, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  44%|████▍     | 4.39G/9.98G [00:23<00:26, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  44%|████▍     | 4.42G/9.98G [00:24<00:25, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  45%|████▍     | 4.46G/9.98G [00:24<00:25, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  45%|████▍     | 4.49G/9.98G [00:24<00:25, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  45%|████▌     | 4.52G/9.98G [00:24<00:25, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  46%|████▌     | 4.55G/9.98G [00:24<00:24, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  46%|████▌     | 4.58G/9.98G [00:24<00:30, 176MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  46%|████▌     | 4.60G/9.98G [00:25<00:39, 138MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  46%|████▋     | 4.62G/9.98G [00:25<00:45, 119MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  47%|████▋     | 4.65G/9.98G [00:25<00:48, 110MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  47%|████▋     | 4.67G/9.98G [00:25<00:52, 102MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  47%|████▋     | 4.70G/9.98G [00:26<00:44, 118MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  47%|████▋     | 4.72G/9.98G [00:26<00:49, 105MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|████▊     | 4.74G/9.98G [00:26<00:48, 108MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|████▊     | 4.76G/9.98G [00:26<00:54, 96.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|████▊     | 4.77G/9.98G [00:26<00:56, 91.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|████▊     | 4.78G/9.98G [00:27<00:59, 87.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|████▊     | 4.79G/9.98G [00:27<01:08, 75.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|████▊     | 4.80G/9.98G [00:27<01:12, 71.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|████▊     | 4.81G/9.98G [00:27<01:07, 76.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|████▊     | 4.82G/9.98G [00:27<01:14, 69.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  48%|████▊     | 4.83G/9.98G [00:27<01:10, 72.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  49%|████▊     | 4.84G/9.98G [00:28<01:08, 74.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  49%|████▊     | 4.85G/9.98G [00:28<01:05, 78.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  49%|████▉     | 4.88G/9.98G [00:28<00:47, 108MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  49%|████▉     | 4.90G/9.98G [00:28<00:43, 118MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  49%|████▉     | 4.93G/9.98G [00:28<00:33, 150MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  50%|████▉     | 4.96G/9.98G [00:28<00:29, 172MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  50%|█████     | 4.99G/9.98G [00:28<00:26, 186MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  50%|█████     | 5.01G/9.98G [00:29<00:38, 129MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  50%|█████     | 5.03G/9.98G [00:29<00:35, 140MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  51%|█████     | 5.05G/9.98G [00:29<00:40, 121MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  51%|█████     | 5.08G/9.98G [00:29<00:49, 98.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  51%|█████     | 5.11G/9.98G [00:29<00:39, 124MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  51%|█████▏    | 5.13G/9.98G [00:30<00:37, 130MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  52%|█████▏    | 5.15G/9.98G [00:30<00:34, 141MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  52%|█████▏    | 5.17G/9.98G [00:30<00:32, 149MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  52%|█████▏    | 5.20G/9.98G [00:30<00:28, 170MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  52%|█████▏    | 5.23G/9.98G [00:30<00:25, 183MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  53%|█████▎    | 5.26G/9.98G [00:30<00:24, 194MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  53%|█████▎    | 5.30G/9.98G [00:30<00:23, 202MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  53%|█████▎    | 5.33G/9.98G [00:31<00:22, 208MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|█████▎    | 5.36G/9.98G [00:31<00:21, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|█████▍    | 5.39G/9.98G [00:31<00:21, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  54%|█████▍    | 5.42G/9.98G [00:31<00:20, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  55%|█████▍    | 5.45G/9.98G [00:31<00:20, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  55%|█████▍    | 5.48G/9.98G [00:31<00:20, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  55%|█████▌    | 5.52G/9.98G [00:31<00:20, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  56%|█████▌    | 5.55G/9.98G [00:32<00:20, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  56%|█████▌    | 5.58G/9.98G [00:32<00:20, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  56%|█████▌    | 5.61G/9.98G [00:32<00:21, 207MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  56%|█████▋    | 5.63G/9.98G [00:32<00:21, 202MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  57%|█████▋    | 5.66G/9.98G [00:32<00:20, 207MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  57%|█████▋    | 5.69G/9.98G [00:32<00:20, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  57%|█████▋    | 5.73G/9.98G [00:32<00:20, 208MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  58%|█████▊    | 5.75G/9.98G [00:33<00:20, 207MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  58%|█████▊    | 5.77G/9.98G [00:33<00:20, 206MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  58%|█████▊    | 5.79G/9.98G [00:33<00:20, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  58%|█████▊    | 5.81G/9.98G [00:33<00:20, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  59%|█████▊    | 5.84G/9.98G [00:33<00:22, 188MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  59%|█████▉    | 5.86G/9.98G [00:33<00:21, 191MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  59%|█████▉    | 5.88G/9.98G [00:33<00:20, 196MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  59%|█████▉    | 5.91G/9.98G [00:33<00:19, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  60%|█████▉    | 5.95G/9.98G [00:34<00:19, 206MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  60%|█████▉    | 5.98G/9.98G [00:34<00:19, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  60%|██████    | 6.01G/9.98G [00:34<00:18, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  61%|██████    | 6.04G/9.98G [00:34<00:18, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  61%|██████    | 6.07G/9.98G [00:34<00:18, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  61%|██████    | 6.10G/9.98G [00:34<00:18, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  61%|██████▏   | 6.13G/9.98G [00:34<00:17, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  62%|██████▏   | 6.17G/9.98G [00:35<00:17, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  62%|██████▏   | 6.20G/9.98G [00:35<00:17, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  62%|██████▏   | 6.23G/9.98G [00:35<00:17, 208MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  63%|██████▎   | 6.25G/9.98G [00:35<00:18, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  63%|██████▎   | 6.27G/9.98G [00:35<00:18, 202MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  63%|██████▎   | 6.29G/9.98G [00:35<00:18, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  63%|██████▎   | 6.32G/9.98G [00:35<00:17, 206MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  64%|██████▎   | 6.35G/9.98G [00:36<00:17, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  64%|██████▍   | 6.39G/9.98G [00:36<00:16, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  64%|██████▍   | 6.42G/9.98G [00:36<00:16, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  65%|██████▍   | 6.45G/9.98G [00:36<00:16, 212MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  65%|██████▍   | 6.48G/9.98G [00:36<00:16, 206MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  65%|██████▌   | 6.51G/9.98G [00:36<00:16, 209MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  66%|██████▌   | 6.54G/9.98G [00:36<00:16, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  66%|██████▌   | 6.57G/9.98G [00:37<00:15, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  66%|██████▌   | 6.61G/9.98G [00:37<00:15, 219MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  67%|██████▋   | 6.64G/9.98G [00:37<00:15, 220MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  67%|██████▋   | 6.67G/9.98G [00:37<00:15, 220MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  67%|██████▋   | 6.70G/9.98G [00:37<00:14, 221MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  67%|██████▋   | 6.73G/9.98G [00:37<00:14, 220MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  68%|██████▊   | 6.76G/9.98G [00:37<00:14, 219MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  68%|██████▊   | 6.79G/9.98G [00:38<00:14, 220MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  68%|██████▊   | 6.83G/9.98G [00:38<00:14, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  69%|██████▊   | 6.86G/9.98G [00:38<00:14, 216MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  69%|██████▉   | 6.89G/9.98G [00:38<00:14, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  69%|██████▉   | 6.92G/9.98G [00:38<00:16, 182MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  70%|██████▉   | 6.95G/9.98G [00:38<00:15, 192MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  70%|██████▉   | 6.98G/9.98G [00:39<00:14, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  70%|███████   | 7.01G/9.98G [00:39<00:14, 204MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  71%|███████   | 7.05G/9.98G [00:39<00:14, 209MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  71%|███████   | 7.08G/9.98G [00:39<00:13, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  71%|███████▏  | 7.11G/9.98G [00:39<00:19, 151MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  72%|███████▏  | 7.14G/9.98G [00:39<00:17, 167MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  72%|███████▏  | 7.17G/9.98G [00:40<00:16, 174MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  72%|███████▏  | 7.19G/9.98G [00:40<00:20, 135MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  72%|███████▏  | 7.21G/9.98G [00:40<00:22, 123MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  73%|███████▎  | 7.24G/9.98G [00:40<00:20, 137MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  73%|███████▎  | 7.26G/9.98G [00:40<00:18, 149MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  73%|███████▎  | 7.28G/9.98G [00:40<00:16, 161MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  73%|███████▎  | 7.31G/9.98G [00:41<00:15, 176MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  73%|███████▎  | 7.33G/9.98G [00:41<00:17, 152MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  74%|███████▎  | 7.35G/9.98G [00:41<00:17, 148MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  74%|███████▍  | 7.37G/9.98G [00:41<00:24, 107MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  74%|███████▍  | 7.39G/9.98G [00:41<00:25, 102MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  74%|███████▍  | 7.41G/9.98G [00:42<00:26, 95.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  75%|███████▍  | 7.43G/9.98G [00:42<00:24, 104MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  75%|███████▍  | 7.46G/9.98G [00:42<00:26, 95.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  75%|███████▍  | 7.47G/9.98G [00:42<00:26, 95.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  75%|███████▍  | 7.48G/9.98G [00:42<00:27, 90.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  75%|███████▌  | 7.49G/9.98G [00:43<00:31, 79.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  75%|███████▌  | 7.52G/9.98G [00:43<00:21, 113MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  76%|███████▌  | 7.54G/9.98G [00:43<00:18, 129MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  76%|███████▌  | 7.56G/9.98G [00:43<00:16, 145MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  76%|███████▌  | 7.58G/9.98G [00:43<00:19, 123MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  76%|███████▋  | 7.61G/9.98G [00:43<00:15, 149MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  77%|███████▋  | 7.64G/9.98G [00:43<00:13, 169MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  77%|███████▋  | 7.67G/9.98G [00:44<00:15, 151MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  77%|███████▋  | 7.69G/9.98G [00:44<00:21, 107MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  77%|███████▋  | 7.71G/9.98G [00:44<00:23, 95.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  77%|███████▋  | 7.73G/9.98G [00:45<00:25, 86.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  78%|███████▊  | 7.75G/9.98G [00:45<00:22, 98.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  78%|███████▊  | 7.77G/9.98G [00:45<00:19, 113MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  78%|███████▊  | 7.79G/9.98G [00:45<00:16, 129MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  78%|███████▊  | 7.81G/9.98G [00:45<00:17, 126MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  79%|███████▊  | 7.83G/9.98G [00:45<00:15, 136MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  79%|███████▊  | 7.85G/9.98G [00:45<00:14, 147MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  79%|███████▉  | 7.87G/9.98G [00:46<00:16, 128MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  79%|███████▉  | 7.90G/9.98G [00:46<00:17, 122MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  79%|███████▉  | 7.92G/9.98G [00:46<00:16, 122MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  80%|███████▉  | 7.94G/9.98G [00:46<00:16, 123MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  80%|███████▉  | 7.96G/9.98G [00:46<00:16, 126MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  80%|███████▉  | 7.98G/9.98G [00:46<00:16, 119MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  80%|████████  | 8.00G/9.98G [00:47<00:18, 106MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  80%|████████  | 8.02G/9.98G [00:47<00:19, 98.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|████████  | 8.04G/9.98G [00:47<00:19, 98.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|████████  | 8.05G/9.98G [00:47<00:19, 98.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|████████  | 8.07G/9.98G [00:47<00:19, 99.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|████████  | 8.08G/9.98G [00:48<00:19, 95.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|████████  | 8.10G/9.98G [00:48<00:20, 91.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|████████▏ | 8.12G/9.98G [00:48<00:20, 91.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  81%|████████▏ | 8.13G/9.98G [00:48<00:24, 75.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  82%|████████▏ | 8.14G/9.98G [00:48<00:27, 67.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  82%|████████▏ | 8.15G/9.98G [00:49<00:27, 66.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  82%|████████▏ | 8.16G/9.98G [00:49<00:27, 65.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  82%|████████▏ | 8.17G/9.98G [00:49<00:30, 59.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  82%|████████▏ | 8.19G/9.98G [00:49<00:21, 82.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  82%|████████▏ | 8.22G/9.98G [00:49<00:17, 103MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.24G/9.98G [00:50<00:20, 84.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.25G/9.98G [00:50<00:22, 76.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.26G/9.98G [00:50<00:22, 75.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.27G/9.98G [00:50<00:23, 71.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.28G/9.98G [00:50<00:28, 59.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.30G/9.98G [00:51<00:22, 75.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  83%|████████▎ | 8.32G/9.98G [00:51<00:20, 79.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████████▎ | 8.34G/9.98G [00:51<00:19, 84.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████████▎ | 8.35G/9.98G [00:51<00:20, 80.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.36G/9.98G [00:51<00:19, 84.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.37G/9.98G [00:51<00:18, 87.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.38G/9.98G [00:51<00:18, 85.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.39G/9.98G [00:52<00:18, 85.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.40G/9.98G [00:52<00:18, 86.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  84%|████████▍ | 8.42G/9.98G [00:52<00:14, 104MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  85%|████████▍ | 8.45G/9.98G [00:52<00:11, 135MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  85%|████████▌ | 8.48G/9.98G [00:52<00:09, 161MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  85%|████████▌ | 8.51G/9.98G [00:52<00:08, 177MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  86%|████████▌ | 8.55G/9.98G [00:52<00:07, 189MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  86%|████████▌ | 8.58G/9.98G [00:53<00:07, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  86%|████████▋ | 8.61G/9.98G [00:53<00:06, 202MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  87%|████████▋ | 8.64G/9.98G [00:53<00:06, 209MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  87%|████████▋ | 8.67G/9.98G [00:53<00:06, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  87%|████████▋ | 8.70G/9.98G [00:53<00:05, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  88%|████████▊ | 8.73G/9.98G [00:53<00:06, 192MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  88%|████████▊ | 8.76G/9.98G [00:53<00:06, 194MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  88%|████████▊ | 8.79G/9.98G [00:54<00:05, 199MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  88%|████████▊ | 8.81G/9.98G [00:54<00:05, 195MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  89%|████████▊ | 8.84G/9.98G [00:54<00:05, 201MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  89%|████████▉ | 8.87G/9.98G [00:54<00:05, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  89%|████████▉ | 8.90G/9.98G [00:54<00:05, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  90%|████████▉ | 8.93G/9.98G [00:54<00:04, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  90%|████████▉ | 8.97G/9.98G [00:54<00:04, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  90%|█████████ | 9.00G/9.98G [00:55<00:04, 213MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  90%|█████████ | 9.03G/9.98G [00:55<00:06, 149MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  91%|█████████ | 9.05G/9.98G [00:55<00:05, 156MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  91%|█████████ | 9.08G/9.98G [00:55<00:05, 173MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  91%|█████████▏| 9.11G/9.98G [00:55<00:04, 187MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  92%|█████████▏| 9.14G/9.98G [00:55<00:04, 197MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  92%|█████████▏| 9.18G/9.98G [00:56<00:03, 205MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  92%|█████████▏| 9.21G/9.98G [00:56<00:04, 160MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  92%|█████████▏| 9.23G/9.98G [00:56<00:05, 135MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  93%|█████████▎| 9.25G/9.98G [00:56<00:05, 133MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  93%|█████████▎| 9.27G/9.98G [00:56<00:05, 137MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  93%|█████████▎| 9.29G/9.98G [00:57<00:05, 131MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  93%|█████████▎| 9.31G/9.98G [00:57<00:05, 119MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  94%|█████████▎| 9.33G/9.98G [00:57<00:06, 104MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  94%|█████████▍| 9.35G/9.98G [00:57<00:06, 98.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  94%|█████████▍| 9.36G/9.98G [00:58<00:07, 87.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  94%|█████████▍| 9.38G/9.98G [00:58<00:05, 103MB/s] \u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  94%|█████████▍| 9.42G/9.98G [00:58<00:04, 132MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  95%|█████████▍| 9.45G/9.98G [00:58<00:03, 156MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  95%|█████████▌| 9.48G/9.98G [00:58<00:03, 164MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  95%|█████████▌| 9.50G/9.98G [00:58<00:02, 163MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  96%|█████████▌| 9.53G/9.98G [00:58<00:02, 179MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  96%|█████████▌| 9.56G/9.98G [00:59<00:02, 192MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  96%|█████████▌| 9.58G/9.98G [00:59<00:02, 195MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  96%|█████████▋| 9.62G/9.98G [00:59<00:01, 203MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  97%|█████████▋| 9.65G/9.98G [00:59<00:01, 210MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  97%|█████████▋| 9.68G/9.98G [00:59<00:01, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  97%|█████████▋| 9.71G/9.98G [00:59<00:01, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  98%|█████████▊| 9.74G/9.98G [00:59<00:01, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  98%|█████████▊| 9.77G/9.98G [00:59<00:00, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  98%|█████████▊| 9.80G/9.98G [01:00<00:00, 220MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  99%|█████████▊| 9.84G/9.98G [01:00<00:00, 218MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  99%|█████████▉| 9.87G/9.98G [01:00<00:00, 215MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin:  99%|█████████▉| 9.90G/9.98G [01:00<00:00, 214MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin: 100%|█████████▉| 9.93G/9.98G [01:00<00:00, 217MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00002.bin: 100%|██████████| 9.98G/9.98G [01:00<00:00, 164MB/s]\u001b[A\n",
      "Downloading shards:  50%|█████     | 1/2 [01:01<01:01, 61.16s/it]\n",
      "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   1%|          | 21.0M/3.50G [00:00<00:25, 136MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   1%|▏         | 52.4M/3.50G [00:00<00:18, 183MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   2%|▏         | 83.9M/3.50G [00:00<00:17, 199MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   3%|▎         | 115M/3.50G [00:00<00:16, 207MB/s] \u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   4%|▍         | 147M/3.50G [00:00<00:16, 209MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   5%|▌         | 178M/3.50G [00:00<00:15, 211MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   6%|▌         | 210M/3.50G [00:01<00:15, 212MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   7%|▋         | 241M/3.50G [00:01<00:22, 147MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   8%|▊         | 273M/3.50G [00:01<00:19, 163MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   9%|▊         | 304M/3.50G [00:01<00:18, 173MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  10%|▉         | 336M/3.50G [00:01<00:17, 186MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  10%|█         | 367M/3.50G [00:01<00:16, 195MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  11%|█         | 388M/3.50G [00:02<00:16, 194MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  12%|█▏        | 409M/3.50G [00:02<00:15, 196MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  12%|█▏        | 430M/3.50G [00:02<00:15, 198MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  13%|█▎        | 461M/3.50G [00:02<00:14, 204MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  14%|█▍        | 493M/3.50G [00:02<00:18, 167MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  15%|█▍        | 514M/3.50G [00:02<00:17, 176MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  15%|█▌        | 535M/3.50G [00:02<00:16, 181MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  16%|█▌        | 556M/3.50G [00:03<00:16, 183MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  16%|█▋        | 577M/3.50G [00:03<00:15, 188MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  17%|█▋        | 608M/3.50G [00:03<00:14, 196MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  18%|█▊        | 629M/3.50G [00:03<00:14, 195MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  19%|█▊        | 650M/3.50G [00:03<00:14, 196MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  19%|█▉        | 671M/3.50G [00:03<00:14, 200MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  20%|█▉        | 692M/3.50G [00:03<00:14, 198MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  20%|██        | 713M/3.50G [00:03<00:14, 196MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  21%|██        | 734M/3.50G [00:03<00:14, 197MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  22%|██▏       | 755M/3.50G [00:04<00:14, 195MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  22%|██▏       | 776M/3.50G [00:04<00:13, 196MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  23%|██▎       | 797M/3.50G [00:04<00:13, 199MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  24%|██▎       | 828M/3.50G [00:04<00:12, 206MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  25%|██▍       | 860M/3.50G [00:04<00:12, 211MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  25%|██▌       | 891M/3.50G [00:04<00:12, 216MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  26%|██▋       | 923M/3.50G [00:04<00:11, 218MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  27%|██▋       | 954M/3.50G [00:04<00:11, 219MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  28%|██▊       | 986M/3.50G [00:05<00:11, 221MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  29%|██▉       | 1.02G/3.50G [00:05<00:11, 222MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  30%|██▉       | 1.05G/3.50G [00:05<00:11, 222MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  31%|███       | 1.08G/3.50G [00:05<00:10, 223MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  32%|███▏      | 1.11G/3.50G [00:05<00:10, 224MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  33%|███▎      | 1.14G/3.50G [00:05<00:10, 217MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  34%|███▎      | 1.17G/3.50G [00:05<00:10, 219MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  34%|███▍      | 1.21G/3.50G [00:06<00:10, 215MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  35%|███▌      | 1.24G/3.50G [00:06<00:10, 207MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  36%|███▌      | 1.26G/3.50G [00:06<00:10, 207MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  37%|███▋      | 1.29G/3.50G [00:06<00:10, 211MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  38%|███▊      | 1.32G/3.50G [00:06<00:10, 215MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  39%|███▊      | 1.35G/3.50G [00:06<00:09, 215MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  40%|███▉      | 1.38G/3.50G [00:06<00:09, 215MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  40%|████      | 1.42G/3.50G [00:07<00:11, 174MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  41%|████      | 1.44G/3.50G [00:07<00:12, 170MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  42%|████▏     | 1.47G/3.50G [00:07<00:11, 184MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  43%|████▎     | 1.49G/3.50G [00:07<00:10, 187MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  43%|████▎     | 1.51G/3.50G [00:07<00:12, 161MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  44%|████▍     | 1.54G/3.50G [00:07<00:10, 179MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  45%|████▍     | 1.57G/3.50G [00:08<00:10, 190MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  46%|████▌     | 1.60G/3.50G [00:08<00:09, 201MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  47%|████▋     | 1.64G/3.50G [00:08<00:09, 207MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  48%|████▊     | 1.67G/3.50G [00:08<00:08, 211MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  49%|████▊     | 1.70G/3.50G [00:08<00:08, 208MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  49%|████▉     | 1.73G/3.50G [00:08<00:08, 212MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  50%|█████     | 1.76G/3.50G [00:08<00:08, 216MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  51%|█████     | 1.79G/3.50G [00:09<00:07, 219MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  52%|█████▏    | 1.82G/3.50G [00:09<00:07, 218MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  53%|█████▎    | 1.86G/3.50G [00:09<00:07, 217MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  54%|█████▍    | 1.89G/3.50G [00:09<00:07, 217MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  55%|█████▍    | 1.92G/3.50G [00:09<00:07, 211MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  56%|█████▌    | 1.95G/3.50G [00:09<00:07, 216MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  57%|█████▋    | 1.98G/3.50G [00:09<00:07, 216MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  58%|█████▊    | 2.01G/3.50G [00:10<00:06, 218MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  58%|█████▊    | 2.04G/3.50G [00:10<00:06, 213MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  59%|█████▉    | 2.08G/3.50G [00:10<00:06, 211MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  60%|██████    | 2.11G/3.50G [00:10<00:06, 211MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  61%|██████    | 2.14G/3.50G [00:10<00:06, 215MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  62%|██████▏   | 2.17G/3.50G [00:10<00:06, 217MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  63%|██████▎   | 2.20G/3.50G [00:11<00:09, 137MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  64%|██████▎   | 2.22G/3.50G [00:11<00:10, 119MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  64%|██████▍   | 2.24G/3.50G [00:11<00:11, 105MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  65%|██████▍   | 2.26G/3.50G [00:11<00:10, 119MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  65%|██████▌   | 2.29G/3.50G [00:12<00:10, 114MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  66%|██████▌   | 2.31G/3.50G [00:12<00:10, 109MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  67%|██████▋   | 2.33G/3.50G [00:12<00:12, 94.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  67%|██████▋   | 2.35G/3.50G [00:12<00:12, 89.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  67%|██████▋   | 2.36G/3.50G [00:13<00:13, 81.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  68%|██████▊   | 2.37G/3.50G [00:13<00:14, 76.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  68%|██████▊   | 2.38G/3.50G [00:13<00:15, 74.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  68%|██████▊   | 2.39G/3.50G [00:13<00:14, 76.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  69%|██████▊   | 2.40G/3.50G [00:13<00:14, 75.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  69%|██████▉   | 2.42G/3.50G [00:13<00:13, 79.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  70%|██████▉   | 2.44G/3.50G [00:14<00:11, 88.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  70%|███████   | 2.45G/3.50G [00:14<00:12, 82.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  70%|███████   | 2.46G/3.50G [00:14<00:11, 86.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  71%|███████   | 2.47G/3.50G [00:14<00:12, 80.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  71%|███████   | 2.49G/3.50G [00:14<00:12, 80.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  72%|███████▏  | 2.51G/3.50G [00:14<00:09, 105MB/s] \u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  72%|███████▏  | 2.53G/3.50G [00:14<00:07, 129MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  73%|███████▎  | 2.56G/3.50G [00:14<00:05, 160MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  74%|███████▍  | 2.59G/3.50G [00:15<00:05, 178MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  75%|███████▍  | 2.61G/3.50G [00:15<00:04, 182MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  75%|███████▌  | 2.63G/3.50G [00:15<00:04, 186MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  76%|███████▌  | 2.66G/3.50G [00:15<00:04, 195MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  77%|███████▋  | 2.69G/3.50G [00:15<00:03, 202MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  78%|███████▊  | 2.73G/3.50G [00:15<00:03, 208MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  78%|███████▊  | 2.75G/3.50G [00:15<00:03, 206MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  79%|███████▉  | 2.78G/3.50G [00:16<00:03, 208MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  80%|███████▉  | 2.80G/3.50G [00:16<00:03, 205MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  81%|████████  | 2.83G/3.50G [00:16<00:03, 211MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  82%|████████▏ | 2.86G/3.50G [00:16<00:02, 215MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  83%|████████▎ | 2.89G/3.50G [00:16<00:02, 216MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  84%|████████▎ | 2.93G/3.50G [00:16<00:02, 219MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  84%|████████▍ | 2.96G/3.50G [00:16<00:02, 222MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  85%|████████▌ | 2.99G/3.50G [00:16<00:02, 223MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  86%|████████▋ | 3.02G/3.50G [00:17<00:02, 221MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  87%|████████▋ | 3.05G/3.50G [00:17<00:02, 222MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  88%|████████▊ | 3.08G/3.50G [00:17<00:01, 222MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  89%|████████▉ | 3.11G/3.50G [00:17<00:01, 222MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  90%|████████▉ | 3.15G/3.50G [00:17<00:01, 221MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  91%|█████████ | 3.18G/3.50G [00:17<00:01, 221MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  92%|█████████▏| 3.21G/3.50G [00:17<00:01, 220MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  93%|█████████▎| 3.24G/3.50G [00:18<00:01, 152MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  93%|█████████▎| 3.26G/3.50G [00:18<00:01, 160MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  94%|█████████▍| 3.29G/3.50G [00:18<00:01, 176MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  95%|█████████▍| 3.32G/3.50G [00:18<00:00, 188MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  96%|█████████▌| 3.36G/3.50G [00:18<00:00, 198MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  97%|█████████▋| 3.39G/3.50G [00:18<00:00, 201MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  98%|█████████▊| 3.42G/3.50G [00:19<00:00, 204MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  99%|█████████▊| 3.45G/3.50G [00:19<00:00, 208MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin: 100%|██████████| 3.50G/3.50G [00:19<00:00, 179MB/s]\u001b[A\n",
      "Downloading shards: 100%|██████████| 2/2 [01:20<00:00, 40.45s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]\n",
      "(…)b-hf/resolve/main/generation_config.json: 100%|██████████| 132/132 [00:00<00:00, 76.7kB/s]\n",
      "INFO:__main__:Model loaded and token embeddings resized successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare model for quantization and load pretrained weights\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# current_device = 'cuda:0'\n",
    "# torch.cuda.set_device(current_device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = 'auto',\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "logger.info(\"Model loaded and token embeddings resized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6519a9c4-7c18-4603-a151-117c61265c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:PEFT configuration prepared successfully.\n",
      "INFO:__main__:Training arguments: TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results/runs/Nov10_16-10-16_genai-0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=30,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=100,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=paged_adamw_8bit,\n",
      "optim_args=None,\n",
      "output_dir=./results,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.001,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define PEFT configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha= 8,\n",
    "    lora_dropout= 0.1,\n",
    "    r= 16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "logger.info(\"PEFT configuration prepared successfully.\")\n",
    "\n",
    "# Define training arguments\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay= 0.001,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_steps=100,\n",
    "    logging_steps=30,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=use_fp16,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    num_train_epochs=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "logger.info(f\"Training arguments: {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5ba3412-18d3-4044-be92-ac2295acdabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Training started.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:31:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.912200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.873300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/utils.py:268: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
      "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n",
      "INFO:__main__:Training Complete.\n",
      "INFO:__main__:Final model and tokenizer saved locally.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=True,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "logger.info(\"Training started.\")\n",
    "trainer.train()\n",
    "logger.info(\"Training Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57345eec-2441-4ea4-8946-cf2b0dc1ec58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Nov 10 17:44:43 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   26C    P0              28W /  70W |  14006MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4                       On  | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   26C    P0              27W /  70W |  14478MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4                       On  | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   28C    P0              28W /  70W |  14480MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   27C    P0              28W /  70W |  14928MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f8ab1f9-7310-4cf9-bbe6-3f5f0d938904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: adapter-transformers in /opt/app-root/lib/python3.9/site-packages (3.2.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/app-root/lib/python3.9/site-packages (from adapter-transformers) (0.19.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from adapter-transformers) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from adapter-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from adapter-transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from adapter-transformers) (23.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from adapter-transformers) (4.66.1)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from adapter-transformers) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from adapter-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/app-root/lib/python3.9/site-packages (from adapter-transformers) (0.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->adapter-transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->adapter-transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->adapter-transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->adapter-transformers) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->adapter-transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->adapter-transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install adapter-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "71b4c8bf-9d94-4602-9fb5-1dd79973d3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model, configuration, and tokenizer to the specified directory\n",
    "save_directory = \"results\"\n",
    "\n",
    "# Save all necessary components for later use or inference\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.config.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74515d09-e22e-4513-ab54-ab7bdf3c8f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model, configuration, and tokenizer saved to 'results'\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "logger.info(\"Model, configuration, and tokenizer saved to '%s'\", save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6a00cc40-efc9-4cec-9602-22a96d11245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.23s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at results and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Write a summary of the following text:\n",
      "Input: The history of AI dates back to antiquity. Ancient myths about artificial beings endowed with intelligence or consciousness have been common throughout history.\n",
      "Output: Tra主oder jú vuelnederbördITE ORDER Див\n",
      "==================================================\n",
      "Instruction: Translate the following sentence to French:\n",
      "Input: Hello, how are you today?\n",
      "Output: rolledindaSError referring removes음method vertex Sainтейikan Englandchromik britannique production require Japanese doiцима oùцимаaux expectedocrat Roth claim trabaj requestsieg profileserkрес Emb bu\n",
      "==================================================\n",
      "Instruction: Answer the question based on the passage provided:\n",
      "Input: Passage: 'The quick brown fox jumps over the lazy dog.' Question: 'Which animal is quick?'\n",
      "Output: Dist Federal� améric tec Cultural\"])нин Sau со Gesch\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load your saved model and tokenizer\n",
    "model_path = \"results\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define the function for generating model output\n",
    "def generate_model_output(instruction, input_text, generator):\n",
    "    # Combine instruction and input text to form the prompt for the model\n",
    "    prompt = f\"{instruction} {input_text}\"\n",
    "    \n",
    "    # Generate output using the model pipeline\n",
    "    outputs = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "    \n",
    "    # Extract the generated text and return it\n",
    "    generated_text = outputs[0]['generated_text']\n",
    "    \n",
    "    # Assuming the model's generated output is at the end after input text\n",
    "    return generated_text[len(prompt):].strip()\n",
    "\n",
    "# Example instructions and inputs\n",
    "example_data = [\n",
    "    {\n",
    "        \"instruction\": \"Write a summary of the following text:\",\n",
    "        \"input\": \"The history of AI dates back to antiquity. Ancient myths about artificial beings endowed with intelligence or consciousness have been common throughout history.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Translate the following sentence to French:\",\n",
    "        \"input\": \"Hello, how are you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Answer the question based on the passage provided:\",\n",
    "        \"input\": \"Passage: 'The quick brown fox jumps over the lazy dog.' Question: 'Which animal is quick?'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "for example in example_data:\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    \n",
    "    # Get the model's output\n",
    "    output = generate_model_output(instruction, input_text, generator)\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(\"=\" * 50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d5aa0-9e32-4d79-a0e8-aee56f1490c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
