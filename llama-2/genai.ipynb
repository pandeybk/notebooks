{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7122956c-3709-4df7-bf9b-7609299ae90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.14.6\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft==0.4.0\n",
      "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m258.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch==2.0.1\n",
      "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.31.0\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m226.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trl==0.4.7\n",
      "  Downloading trl-0.4.7-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m258.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.11.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m201.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m294.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m296.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (23.1)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m196.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-1.26.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m206.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (3.8.5)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m295.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (6.0.1)\n",
      "Collecting dill<0.3.8,>=0.3.0\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2023.10.0,>=2023.1.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m291.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.14.0\n",
      "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m318.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m275.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/app-root/lib/python3.9/site-packages (from datasets==2.14.6->-r requirements.txt (line 1)) (2.31.0)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m210.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.9.5)\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m330.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m255.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m233.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m207.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m339.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (4.7.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m199.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m266.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m201.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m209.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m204.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m334.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m199.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m208.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 3)) (3.1.2)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m202.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m207.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m211.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.3/773.3 kB\u001b[0m \u001b[31m327.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m221.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 3)) (53.0.0)\n",
      "Requirement already satisfied: wheel in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 3)) (0.38.4)\n",
      "Collecting lit\n",
      "  Downloading lit-17.0.4.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m290.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m211.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets==2.14.6->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.14.6->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets==2.14.6->-r requirements.txt (line 1)) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m313.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m315.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m317.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6->-r requirements.txt (line 1)) (1.16.0)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-17.0.4-py3-none-any.whl size=93257 sha256=3089a5758ea1b94024b2e5cf70841b903e1234a16deb97334b46fcbed67c2713\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zoozwn9k/wheels/65/b5/5f/393c25945f561fcc619d4ce4ca365020675dbddb0c92cb5ef6\n",
      "Successfully built lit\n",
      "Installing collected packages: tokenizers, pytz, mpmath, lit, cmake, xxhash, tzdata, tqdm, sympy, safetensors, regex, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, fsspec, filelock, dill, tiktoken, scipy, pyarrow, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, huggingface-hub, transformers, datasets, triton, torch, accelerate, trl, peft\n",
      "Successfully installed accelerate-0.24.1 cmake-3.27.7 datasets-2.14.6 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.0 lit-17.0.4 mpmath-1.3.0 multiprocess-0.70.15 networkx-3.2.1 numpy-1.26.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-2.1.2 peft-0.4.0 pyarrow-14.0.1 pytz-2023.3.post1 regex-2023.10.3 safetensors-0.4.0 scipy-1.11.3 sympy-1.12 tiktoken-0.5.1 tokenizers-0.13.3 torch-2.0.1 tqdm-4.66.1 transformers-4.31.0 triton-2.0.0 trl-0.4.7 tzdata-2023.3 xxhash-3.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4af30d-3460-4166-abef-b55fe514fce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1c6705-2ec5-4fc8-86d5-49052b99aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153bae2d-d87f-4272-8f10-6fcb5be94c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # \"True\" is to restart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291bc48e-c45c-4d3e-9e4d-28269eda4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  9 19:46:36 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   40C    P0              69W /  70W |  14662MiB / 15360MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4                       On  | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   24C    P0              27W /  70W |   4416MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4                       On  | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   26C    P0              28W /  70W |   4416MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   25C    P0              28W /  70W |   5416MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Device Name (GPU 0): Tesla T4\n",
      "Total CUDA memory on GPU 0: 14.58 GB\n",
      "Used CUDA memory on GPU 0: 0.00 GB\n",
      "Used CUDA memory on GPU: OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])\n",
      "Reserved CUDA memory on GPU 0: 0.00 GB\n",
      "\n",
      "Device Name (GPU 1): Tesla T4\n",
      "Total CUDA memory on GPU 1: 14.58 GB\n",
      "Used CUDA memory on GPU 1: 0.00 GB\n",
      "Used CUDA memory on GPU: OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])\n",
      "Reserved CUDA memory on GPU 1: 0.00 GB\n",
      "\n",
      "Device Name (GPU 2): Tesla T4\n",
      "Total CUDA memory on GPU 2: 14.58 GB\n",
      "Used CUDA memory on GPU 2: 0.00 GB\n",
      "Used CUDA memory on GPU: OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])\n",
      "Reserved CUDA memory on GPU 2: 0.00 GB\n",
      "\n",
      "Device Name (GPU 3): Tesla T4\n",
      "Total CUDA memory on GPU 3: 14.58 GB\n",
      "Used CUDA memory on GPU 3: 0.00 GB\n",
      "Used CUDA memory on GPU: OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])\n",
      "Reserved CUDA memory on GPU 3: 0.00 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "\n",
    "# Loop over all available GPUs\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device Name (GPU {i}): {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # Get the total memory of the current GPU\n",
    "    total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "    print(f'Total CUDA memory on GPU {i}: {total_memory:.2f} GB')\n",
    "\n",
    "    # Get the memory currently allocated on the current GPU\n",
    "    allocated_memory = torch.cuda.memory_allocated(i) / 1024**3\n",
    "    print(f'Used CUDA memory on GPU {i}: {allocated_memory:.2f} GB')\n",
    "    \n",
    "    # Get the memory currently allocated on the current GPU\n",
    "    used_memory = torch.cuda.memory_stats(i)\n",
    "    print(f'Used CUDA memory on GPU: {used_memory}')\n",
    "\n",
    "    # Get the memory currently reserved on the current GPU\n",
    "    reserved_memory = torch.cuda.memory_reserved(i) / 1024**3\n",
    "    print(f'Reserved CUDA memory on GPU {i}: {reserved_memory:.2f} GB')\n",
    "    \n",
    "    print()  # Print a newline for better readability between GPU reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ca3c81-5540-4606-9efa-f37721c09f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Available CUDA devices: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n"
     ]
    }
   ],
   "source": [
    "# Check for available GPU devices and list them\n",
    "if torch.cuda.is_available():\n",
    "    available_gpus = [f'cuda:{i}' for i in range(torch.cuda.device_count())]\n",
    "    logger.info(f\"Available CUDA devices: {available_gpus}\")\n",
    "else:\n",
    "    logger.warning(\"No CUDA devices available. Using CPU.\")\n",
    "    available_gpus = ['cpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3a9550-6d80-41c9-99b2-cb743c6d9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "logger.info(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3832915d-6237-4736-9037-024b1b242be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Tokenizer prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"daryl149/llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "logger.info(\"Tokenizer prepared successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8166da-6f27-4a70-9922-fc02da92107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]\n",
      "INFO:__main__:Model loaded and token embeddings resized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for quantization and load pretrained weights\n",
    "quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"daryl149/llama-2-7b-chat-hf\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    # Remove the device_map argument since we are using DataParallel\n",
    "\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "logger.info(\"Model loaded and token embeddings resized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b92aba2-8641-47b9-9268-1766ada0c383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using 4 GPUs for DataParallel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # If we have multiple GPUs, wrap the model with nn.DataParallel\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     logger.info(f\"Using {torch.cuda.device_count()} GPUs for DataParallel\")\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "# # Move the model to GPU\n",
    "# model = model.to('cuda:2')  # DataParallel will automatically use the other GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913eb41b-c063-4984-a21a-2944c6f2997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model Prepared for kbit training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "logger.info(\"Model Prepared for kbit training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de47f926-d058-471e-bbc4-4cc67800f130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:PEFT configuration prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define PEFT configuration\n",
    "peft_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "logger.info(\"PEFT configuration prepared successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1cc2094-f33d-4987-874e-ae0ba75c6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/training_args.py:1617: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Training arguments: TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=16,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=llama-finetuned-7b2/runs/Nov08_23-43-36_genai-0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=llama-finetuned-7b2,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=llama-finetuned-7b2,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama-finetuned-7b2\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=use_fp16,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    num_train_epochs=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    push_to_hub_token=\"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\",\n",
    ")\n",
    "logger.info(f\"Training arguments: {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ed38a63-91d1-4811-8cf3-b64f08455701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "INFO:__main__:Trainer initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=True,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "logger.info(\"Trainer initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e3c3f9c-159c-4af6-8752-d2da5f2238d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedDataParallel \u001b[38;5;28;01mas\u001b[39;00m DDP\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mDDP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Now, when you call methods on this model, it will spread the work across the GPUs more efficiently.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/torch/nn/parallel/distributed.py:625\u001b[0m, in \u001b[0;36mDistributedDataParallel.__init__\u001b[0;34m(self, module, device_ids, output_device, dim, broadcast_buffers, process_group, bucket_cap_mb, find_unused_parameters, check_reduction, gradient_as_bucket_view, static_graph)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device \u001b[38;5;241m=\u001b[39m _get_device_index(output_device, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_group \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_group \u001b[38;5;241m=\u001b[39m process_group\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/torch/distributed/distributed_c10d.py:707\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03mGetting the default process group created by init_process_group\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    710\u001b[0m     )\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "# Clear any cached memory to free up as much GPU memory as possible\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize DDP if you're using multiple GPUs\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DDP(model)\n",
    "\n",
    "# Now, when you call methods on this model, it will spread the work across the GPUs more efficiently.\n",
    "trainer.train()\n",
    "logger.info(\"Training started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f675720-1fd0-464b-bc3d-db40d80a1607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name (GPU 0): Tesla T4\n",
      "Total CUDA memory on GPU 0: 14.58 GB\n",
      "Used CUDA memory on GPU 0: 0.00 GB\n",
      "Reserved CUDA memory on GPU 0: 0.00 GB\n",
      "\n",
      "Device Name (GPU 1): Tesla T4\n",
      "Total CUDA memory on GPU 1: 14.58 GB\n",
      "Used CUDA memory on GPU 1: 0.00 GB\n",
      "Reserved CUDA memory on GPU 1: 0.00 GB\n",
      "\n",
      "Device Name (GPU 2): Tesla T4\n",
      "Total CUDA memory on GPU 2: 14.58 GB\n",
      "Used CUDA memory on GPU 2: 0.00 GB\n",
      "Reserved CUDA memory on GPU 2: 0.00 GB\n",
      "\n",
      "Device Name (GPU 3): Tesla T4\n",
      "Total CUDA memory on GPU 3: 14.58 GB\n",
      "Used CUDA memory on GPU 3: 0.00 GB\n",
      "Reserved CUDA memory on GPU 3: 0.00 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Loop over all available GPUs\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device Name (GPU {i}): {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # Get the total memory of the current GPU\n",
    "    total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "    print(f'Total CUDA memory on GPU {i}: {total_memory:.2f} GB')\n",
    "\n",
    "    # Get the memory currently allocated on the current GPU\n",
    "    allocated_memory = torch.cuda.memory_allocated(i) / 1024**3\n",
    "    print(f'Used CUDA memory on GPU {i}: {allocated_memory:.2f} GB')\n",
    "\n",
    "    # Get the memory currently reserved on the current GPU\n",
    "    reserved_memory = torch.cuda.memory_reserved(i) / 1024**3\n",
    "    print(f'Reserved CUDA memory on GPU {i}: {reserved_memory:.2f} GB')\n",
    "    \n",
    "    print()  # Print a newline for better readability between GPU reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b45d31c-be7b-4ee3-b642-e8e0eece3976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  9 21:02:45 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   40C    P0              72W /  70W |  14662MiB / 15360MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4                       On  | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   23C    P0              27W /  70W |   4416MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4                       On  | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   26C    P0              28W /  70W |   4416MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   25C    P0              27W /  70W |   5416MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b676a2b-12ab-446e-a8b4-5697d9067a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
